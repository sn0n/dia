{
  "id": "rlhf-reinforcement-learning-from-human-feedback",
  "title": "RLHF (Reinforcement Learning from Human Feedback)",
  "type": "ai-workflows",
  "category": "Alignment",
  "description": "Generate → Human Ranks → Train Reward Model → PPO Training → Aligned Model",
  "slug": "rlhf-reinforcement-learning-from-human-feedback-diagram",
  "url": "/ai-workflows/rlhf-reinforcement-learning-from-human-feedback-diagram",
  "hasAdvanced": false,
  "source": null,
  "steps": [
    "Generate",
    "Human Ranks",
    "Train Reward Model",
    "PPO Training",
    "Aligned Model"
  ],
  "advanced": null
}